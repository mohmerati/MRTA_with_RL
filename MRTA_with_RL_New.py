# -*- coding: utf-8 -*-
"""MRTA_with_RL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DclFEZiFOrRdgro4uzB8GlDHrFGtw-5X
"""

# !pip install stable_baselines3

import time
import numpy as np
from typing import Dict, Any, List, Optional, Callable
import gymnasium as gym
from gymnasium import spaces
import torch
import torch.nn as nn
import torch.nn.functional as F
from stable_baselines3.common.policies import ActorCriticPolicy
from stable_baselines3.common.preprocessing import preprocess_obs
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv, VecNormalize
from stable_baselines3.common.utils import set_random_seed
from stable_baselines3 import PPO
from stable_baselines3.common.distributions import MultiCategoricalDistribution
import shutil, os, glob
from stable_baselines3.common.callbacks import BaseCallback
from collections import deque


class QueueEnv(gym.Env):
    """
    Multi-robot, multi-queue env with switching delay = 1.
    Actions per robot: 0=idle, 1=serve, 2..N+1=switch_to(k=action-2).
    Observation: dict(robots: positions, queues: lengths).
    Reward: -sum(queues).
    """
    metadata = {"render_modes": []}

    def __init__(self,
                 M: int = 2,
                 N: int = 4,
                 arrival_params=None,
                 seed: Optional[int] = None,
                 lambda_collision: float = 0.0,
                 max_steps_per_run: Optional[int] = None,
                 max_queue_length: int = 5):          # <<< NEW
        super().__init__()
        self.M, self.N = M, N
        self.lambda_collision = float(lambda_collision)
        self.max_steps_per_run = max_steps_per_run                 # <<< NEW

        if arrival_params is None:
            arrival_params = np.full(N, 0.10, dtype=np.float32)
        self.arrival_params = np.asarray(arrival_params, dtype=np.float32)

        # RNG and step counter
        self._seed = None
        self.rng = np.random.default_rng(seed)
        self._step_count = 0                                       # <<< NEW

        # State
        self._queues = np.zeros(self.N, dtype=np.float32)
        self._robots = np.arange(self.M, dtype=np.int64)

        # Spaces
        self.observation_space = spaces.Dict({
            "robots": spaces.MultiDiscrete([N] * M),
            "queues": spaces.Box(low=0, high=max_queue_length, shape=(N,), dtype=np.float32)
        })
        self.action_space = spaces.MultiDiscrete([N] * M)

    def _get_obs(self):
        return {"robots": self._robots.copy(), "queues": self._queues.copy()}

    def reset(self, *, seed: Optional[int] = None, options: Optional[Dict[str, Any]] = None):
        # Gymnasium seeding API
        if seed is not None:
            self._seed = int(seed)
            self.rng = np.random.default_rng(self._seed)
        self._step_count = 0                                       # <<< reset counter
        self._queues.fill(0.0)
        self._robots = np.arange(self.M, dtype=np.int64)
        obs = self._get_obs()
        info: Dict[str, Any] = {}
        return obs, info

    def _resolve_collisions(self, intended_pos):
        # Reserve all current positions initially
        final_pos = self._robots.copy()
        taken = set(final_pos.tolist())

        projected = 0
        for r in range(self.M):
            tgt = int(intended_pos[r])
            cur = int(self._robots[r])

            if tgt == cur:
                # Stayer: already reserved; nothing to do
                continue

            if tgt not in taken:
                # Move succeeds: free old spot, claim target
                final_pos[r] = tgt
                taken.remove(cur)
                taken.add(tgt)
            else:
                # Move fails: stay; count projection
                projected += 1
                # final_pos[r] already equals cur; 'taken' unchanged

        return final_pos, projected

    def step(self, action: np.ndarray):
        action = np.asarray(action, dtype=np.int64)
        assert action.shape == (self.M,), f"Expected action shape {(self.M,)}, got {action.shape}"

        # --- snapshot BEFORE
        queues_before = self._queues.copy()
        robots_before = self._robots.copy()

        # --- reward from current queues (pre-cost)
        qsmean = float(self._queues.mean())

        cost_pre = float(self._queues.sum())
        reward = -cost_pre

        # --- serve (record who actually served)
        served_mask = np.zeros(self.M, dtype=np.int64)
        for r in range(self.M):
            loc = int(self._robots[r])
            if action[r] == loc and self._queues[loc] > 0:
                self._queues[loc] -= 1.0
                served_mask[r] = 1

        # --- switch intentions
        intended = self._robots.copy()
        for r in range(self.M):
            intended[r] = action[r]

        # --- apply collision resolution
        new_pos, num_proj = self._resolve_collisions(intended)
        self._robots = new_pos

        # --- arrivals
        p = self.arrival_params
        arrivals = self.rng.binomial(n=1, p=p, size=self.N).astype(np.float32)
        self._queues = np.clip(self._queues + arrivals, 0.0, 500)

        # --- collision penalty
        if self.lambda_collision > 0 and num_proj > 0:
            reward -= self.lambda_collision * float(num_proj)

        # --- truncation logic
        self._step_count += 1
        truncated = (
            self.max_steps_per_run is not None
            and self._step_count >= self.max_steps_per_run
        )
        terminated = False
        obs = self._get_obs()
        # print(f"queues_mean: {qsmean}, collisions: {int(num_proj)}")  # <<< DEBUG PRINT
        info: Dict[str, Any] = {
            "queues_mean": qsmean,
            "collisions_count": int(num_proj),
        }

        # --- TRACE: rich info for debugging/analysis
        # info: Dict[str, Any] = {
        #     "queues_before": queues_before,
        #     "robots_before": robots_before,
        #     "action_array": action.copy(),
        #     "served_mask": served_mask,
        #     "intended_positions": intended,
        #     "robots_after": self._robots.copy(),
        #     "arrivals": arrivals,
        #     "num_projections": int(num_proj),
        #     "cost_pre": cost_pre,
        #     "reward": reward,
        #     "truncated": truncated,
        # }
        return obs, reward, terminated, truncated, info

# PolicyFn = Callable[[Dict[str, np.ndarray]], np.ndarray]

# def rollout_with_trace(
#     env: "QueueEnv",
#     policy: PolicyFn,
#     T: int = 50,
#     seed: Optional[int] = 0,
#     reset: bool = True,
#     verbose: bool = True,
# ) -> List[Dict[str, Any]]:
#     """
#     Runs T steps, logging pre/post state, arrivals, service, moves, reward, truncation.
#     Returns a list of per-step dict records (including info from env.step()).
#     """
#     records: List[Dict[str, Any]] = []

#     if reset:
#         obs, info = env.reset(seed=seed)
#     else:
#         obs = env._get_obs()

#     for t in range(T):
#         # Policy picks joint action given current observation
#         action = policy(obs)  # shape [M], int64: 0=idle,1=serve,2..=switch_to(k)

#         # One environment tick
#         obs_next, reward, terminated, truncated, info = env.step(action)

#         # Pretty print one-line trace
#         if verbose:
#             qb = info["queues_before"]
#             rb = info["robots_before"]
#             arr = info["arrivals"]
#             sm = info["served_mask"]
#             inten = info["intended_positions"]
#             ra = info["robots_after"]
#             print(
#                 f"t={t:03d}   robots:{rb}  queues:{qb}  a:{action}  "
#                 f"serve:{sm}  intend:{inten}  robots'→{ra}  "
#                 f"arr:{arr.astype(int)}  pre_cost:{info['cost_pre']:.1f}  "
#                 f"r:{reward:.1f}  proj:{info['num_projections']}  trunc:{truncated}"
#             )

#         # Collect full record
#         rec = {
#             "t": t,
#             "obs_before": {"robots": info["robots_before"], "queues": info["queues_before"]},
#             "action": action.copy(),
#             "served_mask": info["served_mask"],
#             "intended_positions": info["intended_positions"],
#             "arrivals": info["arrivals"],
#             "robots_after": info["robots_after"],
#             "obs_after": obs_next,
#             "reward": reward,
#             "cost_pre": info["cost_pre"],
#             "num_projections": info["num_projections"],
#             "terminated": terminated,
#             "truncated": truncated,
#         }
#         records.append(rec)

#         obs = obs_next
#         if terminated or truncated:
#             # Reset to continue trace smoothly (optional)
#             obs, _ = env.reset()
#             if verbose:
#                 print("— episode boundary — reset —")

#     return records

# def random_policy(obs: Dict[str, np.ndarray]) -> np.ndarray:
#     """Uniform random over {idle, serve, switch-to-k} per robot."""
#     robots = obs["robots"]
#     queues = obs["queues"]
#     M = robots.shape[-1]
#     N = queues.shape[-1]
#     a = np.empty(M, dtype=np.int64)
#     for r in range(M):
#         a[r] = np.random.randint(0, N)  # 0..N-1
#     return a

# def greedy_local_or_switch_to_max(obs: Dict[str, np.ndarray]) -> np.ndarray:
#     """
#     If there is work at robot's current queue, serve.
#     Else switch to the queue with maximal length (ties → smallest index).
#     If all zero, idle.
#     """
#     robots = obs["robots"]
#     queues = obs["queues"]
#     M = robots.shape[-1]
#     N = queues.shape[-1]
#     a = np.zeros(M, dtype=np.int64)
#     argmax_q = int(np.argmax(queues))
#     for r in range(M):
#         loc = int(robots[r])
#         if queues[loc] > 0:
#             a[r] = loc # serve
#         elif queues.max() > 0:
#             a[r] = argmax_q  # switch to global max queue
#         else:
#             a[r] = loc # idle
#     return a

# # Example: small env to see events clearly
# env = QueueEnv(
#     M=1, N=3,
#     arrival_params={"p": np.array([0.2, 0.1, 0.5], dtype=np.float32)},
#     lambda_collision=3.0,
#     max_steps_per_run=10,   # small horizon so you see truncation quickly
#     seed=123,
# )

# print("=== Greedy-local-or-switch-to-max (expect serves & occasional collisions) ===")
# _ = rollout_with_trace(env, greedy_local_or_switch_to_max, T=25, seed=123, verbose=True)

# print("\n=== Random policy (expect more collisions and noisy rewards) ===")
# _ = rollout_with_trace(env, random_policy, T=20, seed=999, verbose=True)

class ReLUN(nn.Module):
    def __init__(self, Ncaps):
        super().__init__()
        self.N = float(Ncaps)

    def forward(self, x):
        return torch.clamp(x, 0.0, self.N)

class PosLinear(nn.Module):
    def __init__(self, in_features, out_features, eps=1e-6):
        super().__init__()
        self.W_param = nn.Parameter(torch.empty(out_features, in_features))
        self.b_param = nn.Parameter(torch.zeros(out_features))
        nn.init.xavier_uniform_(self.W_param)
        self.eps = eps
    def forward(self, x):
        # W_pos = torch.exp(self.W_param) + self.eps
        W_pos = torch.exp(self.W_param)
        return x @ W_pos.T + self.b_param

class MonotoneMLP(nn.Module):
    def __init__(self, in_features, hidden_dims, Ncaps):
        super().__init__()
        layers = []
        prev = in_features
        for hidden_dim in hidden_dims:
            layers.append(PosLinear(prev, hidden_dim))
            layers.append(ReLUN(Ncaps))
            prev = hidden_dim
        layers.append(PosLinear(prev, 1))
        self.net = nn.Sequential(*layers)

    def forward(self, x):
        return self.net(x)

class PsiSwitch(nn.Module):
    def __init__(self, in_dim, hid=64):
        super().__init__()
        self.mlp = nn.Sequential(
            nn.Linear(in_dim, hid), nn.ReLU(),
            nn.Linear(hid, hid//2), nn.ReLU(),
            nn.Linear(hid//2, 1),
        )

    def forward(self, z_flat):
        return self.mlp(z_flat)

class QueuePolicyNet(nn.Module):
    def __init__(self, M, N, d_r=16,
                 arrival_rates=None,
                 Ncaps=20,
                 mono_hidden=[32, 32],
                 head_hid=64
                 ):
        super().__init__()
        self.M = M
        self.N = N
        self.d_r = d_r
        self.register_buffer("rates", arrival_rates)
        self.mono_mlp = MonotoneMLP(2, mono_hidden, Ncaps)
        self.robot_emb = nn.Embedding(num_embeddings=self.N, embedding_dim=d_r)
        # Pairwise feature dim D = u(1) + e(d_r) + d(1) + c(2) = d_r + 4
        D = d_r + 4
        self.psi_sw = PsiSwitch(D, hid=head_hid)
        self.v_head = nn.Sequential(
            nn.Linear(1+d_r+2, head_hid), nn.ReLU(),
            nn.Linear(head_hid, head_hid), nn.ReLU(),
            nn.Linear(head_hid, 1)
        )

    def _queue_urgency(self, lengths):
        B = lengths.shape[0]
        rates = self.rates.view(1, self.N).expand(B, self.N)
        mono_in = torch.stack([lengths, rates], dim=-1)
        u = self.mono_mlp(mono_in.view(B*self.N, 2)).view(B, self.N, 1)
        return u

    def forward(self, obs):
        dev = next(self.parameters()).device
        N, M, d_r = self.N, self.M, self.d_r
        lengths = obs["queues"].float()                                 #[B, N]
        # print(f"length shape: {lengths.shape}")
        robots = obs["robots"].long()                                   #[B, M]
        # print(f"s shape: {robots.shape}")
        B = lengths.shape[0]
        u = self._queue_urgency(lengths)                                #[B, N, 1]
        # print(f"u shape: {u.shape}")
        e = self.robot_emb(robots)                                      #[B, M, d_r]
        # print(f"e shape: {e.shape}")
        mean_u = u.mean(dim=1)
        # print(f"mean u shape: {mean_u.shape}")
        max_u = u.max(dim=1).values
        c = torch.cat([mean_u,max_u], dim=-1)                           #[B, 2]
        # print(f"c shape: {c.shape}")
        u_expand = u.unsqueeze(1).expand(B, M, N, 1)                    #[B, M, N, 1]
        # print(f"u_expand shape: {u_expand.shape}")
        e_expand = e.unsqueeze(2).expand(B, M, N, d_r)                  #[B, M, N, d_r]
        # print(f"e_expand shape: {e_expand.shape}")
        c_expand = c.view(B,1,1,2).expand(B, M, N, 2)                   #[B, M, N, 2]
        i_idx = torch.arange(N, device=dev).view(1,1,N).expand(B, M, N) #[B,M,N]
        s_mat = robots.view(B, M, 1).expand(B, M, N)                    #[B,M,N]
        dflag = (i_idx != s_mat).float().unsqueeze(-1)                  #[B,M,N,1]
        Z = torch.cat([u_expand, e_expand, dflag, c_expand], dim=-1)    #[B,M,N,d_r+4]
        # Switch logits
        D = d_r + 4
        switch_logits = self.psi_sw(Z.view(B*M*N, D)).view(B, M, N)     #[B,M,N]
        # Centralized value
        e_mean = e.mean(dim=1, keepdim=True)                            #[B,1,d_r]
        v_in = torch.cat([mean_u.view(B,1), e_mean.squeeze(1),c],dim=-1)#[B,1+d_r+2]
        value = self.v_head(v_in).squeeze(-1)                           #[B,1]
        return switch_logits, value

class QueuePPOPolicy(ActorCriticPolicy):
    def __init__(self, observation_space, action_space, lr_schedule, arrival_rates, **kwargs):
        super().__init__(observation_space, action_space, lr_schedule, **kwargs)
        # Derive M and N from MultiDiscrete
        self.M = int(len(action_space.nvec))
        self.N = int(action_space.nvec[0])
        arrival_rates = torch.as_tensor(arrival_rates, dtype=torch.float32, device=self.device)
        # Your domain network (actor+critic)
        self.net = QueuePolicyNet(
            M=self.M, N=self.N, d_r=16,
            arrival_rates=arrival_rates,
            Ncaps=200,
            mono_hidden=[32, 32],
            head_hid=64,
        )
        # SB3-compatible multi-categorical over each robot's action head
        self._action_dims = action_space.nvec.tolist()
        self._dist = MultiCategoricalDistribution(self._action_dims)

    # === IMPORTANT: provide a dummy mlp_extractor so SB3 is happy ===
    def _build_mlp_extractor(self) -> None:
        class _Bypass(nn.Module):
            def __init__(self):
                super().__init__()
                # SB3 looks for these two attributes to size default heads;
                # we give harmless nonzero placeholders.
                self.latent_dim_pi = 1
                self.latent_dim_vf = 1
            def forward(self, features: torch.Tensor):
                # Not used by our policy; return dummy tensors with correct batch dim
                b = features.shape[0] if features.ndim > 0 else 1
                z = torch.zeros((b, 1), device=features.device, dtype=features.dtype)
                return z, z
        self.mlp_extractor = _Bypass()

    def forward(self, obs, deterministic: bool = False):
        """
        Return: action [B, M], value [B], log_prob [B]
        Uses SB3's MultiCategoricalDistribution so that .predict() and training paths agree.
        """
        logits, value = self.net(obs)                     # [B, M, N], [B]
        B = logits.shape[0]
        # Flatten per-robot logits for MultiCategoricalDistribution: [B, sum(nvec)] = [B, M*N]
        logits_flat = logits.view(B, -1)
        dist = self._dist.proba_distribution(logits_flat)
        action = dist.get_actions(deterministic=deterministic)   # [B, M]
        log_prob = dist.log_prob(action)                         # [B]
        return action, value, log_prob

    # SB3 uses this during updates to rebuild the distribution
    def get_distribution(self, obs):
        """
        Build an SB3 MultiCategoricalDistribution from the network logits.
        Returns an object with .get_actions(), .log_prob(), .entropy().
        """
        logits, _ = self.net(obs)               # [B, M, N]
        B = logits.shape[0]
        logits_flat = logits.view(B, -1)        # [B, sum(nvec)]
        return self._dist.proba_distribution(logits_flat)


    def predict_values(self, obs):
        """
        Get the estimated values according to the current policy.
        :param obs:
        :return: the estimated values.
        """
        # Pass the dictionary observation directly to the network's forward_critic
        return self.forward_critic(obs)


    # Used in PPO training loop to compute log-probs/entropy on minibatches
    def evaluate_actions(self, obs, actions):
        """
        obs: dict batch (from rollout buffer)
        actions: LongTensor of shape [batch, M]
        Return: values [batch], log_prob [batch], entropy [batch]
        """
        logits, value = self.net(obs)                 # [B, M, N], [B]
        B = logits.shape[0]
        logits_flat = logits.view(B, -1)              # [B, sum(nvec)]
        dist = self._dist.proba_distribution(logits_flat)
        log_prob = dist.log_prob(actions)             # [B]
        entropy = dist.entropy()                      # [B]
        return value, log_prob, entropy

    # Optional, used by SB3 in some code paths
    def forward_actor(self, obs):
        logits, _ = self.net(obs)
        return logits

    def forward_critic(self, obs):
        _, value = self.net(obs)
        return value

# from stable_baselines3.common.env_checker import check_env
# from stable_baselines3 import PPO

# env = QueueEnv(M=2, N=4, arrival_params={"p": [0.1, 0.2, 0.15, 0.3]},
#                lambda_collision=0.0,
#                max_steps_per_run=10_000)
# check_env(env, warn=True)
# arrival_rates = torch.tensor(env.arrival_params["p"], dtype=torch.float32)
# device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# model = PPO(
#     QueuePPOPolicy,
#     env,
#     learning_rate=3e-4,
#     n_steps=512,
#     batch_size=64,
#     ent_coef=0.01,
#     vf_coef=0.5,
#     max_grad_norm=0.5,
#     verbose=1,
#     device=device,
#     policy_kwargs=dict(arrival_rates=arrival_rates),
# )

def make_queue_env(rank: int,
                   base_seed: int,
                   **env_kwargs):
    """
    rank: worker id in [0, n_envs)
    base_seed: master seed for reproducibility
    env_kwargs: kwargs forwarded to QueueEnv(...)
    """
    def _init():
        env = QueueEnv(**env_kwargs)
        env = Monitor(env, info_keywords=("queues_mean", "collisions_count"))  # logs ep stats for TensorBoard/evaluation
        # Set a distinct, reproducible seed per worker
        worker_seed = base_seed + rank
        env.reset(seed=worker_seed)  # Gymnasium seeding API
        return env
    return _init

n_envs = 8
base_seed = 123

env_kwargs = dict(
    M=2, N=4,
    arrival_params= np.array([0.1, 0.1, 0.1, 0.1], dtype=np.float32),
    lambda_collision=0.0,
    max_steps_per_run=10_000,
)

vec_env = DummyVecEnv([make_queue_env(i, base_seed, **env_kwargs) for i in range(n_envs)])
# env = VecNormalize(vec_env, norm_obs=True, norm_reward=True, clip_obs=10.)

set_random_seed(base_seed, using_cuda=torch.cuda.is_available())

# from google.colab import drive
# drive.mount('/content/drive')

arrival_rates = torch.tensor(env_kwargs["arrival_params"], dtype=torch.float32)

current_dir = os.getcwd()
tensorboard_dir = current_dir + "/tb_queueppo"
# print("TensorBoard log dir:", tensorboard_dir)
# print(torch.backends.mps.is_available())  # True if MPS backend works
# print(torch.backends.mps.is_built())      # True if your torch build supports it
# device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")
device = "cpu"
learning_rate=3e-4
n_steps=64
batch_size=32
n_epochs=5
ent_coef=0.02
vf_coef=0.5
max_grad_norm=0.5

model = PPO(
    QueuePPOPolicy,
    vec_env,                           # vectorized env here
    learning_rate=learning_rate,
    n_steps=n_steps,                       # steps *per env* before each update
    batch_size=batch_size,
    n_epochs=n_epochs,                       # PPO SGD passes per update
    ent_coef=ent_coef,
    vf_coef=vf_coef,
    max_grad_norm=max_grad_norm,
    device=device,
    tensorboard_log=tensorboard_dir,
    policy_kwargs=dict(arrival_rates=arrival_rates),
    verbose=1,)

class RolloutInfoMean(BaseCallback):
    """
    Compute per-rollout means of selected `info` keys.
    Uses *only* the samples collected in the current rollout window
    (n_envs * n_steps), then resets for the next rollout.
    """
    def __init__(self, keys=("queues_mean", "collisions_count"), log_prefix="rollout"):
        super().__init__()
        self.keys = tuple(keys)
        self.log_prefix = log_prefix
        self._reset_accums()

    def _reset_accums(self):
        # one scalar accumulator per key
        self._sum = {k: 0.0 for k in self.keys}
        self._count = 0

    def _on_rollout_start(self) -> None:
        # called once before collecting n_steps for each env
        self._reset_accums()

    def _on_step(self) -> bool:
        # called at every vectorized step; infos is a list of length n_envs
        infos = self.locals.get("infos", [])
        for info in infos:
            if not info:
                continue
            for k in self.keys:
                if k in info:
                    # cast to float to avoid dtype surprises from numpy scalars
                    self._sum[k] += float(info[k])
                    # count one sample for each key present
        # increment count by number of envs that produced an `info`
        self._count += len(infos)
        return True

    def _on_rollout_end(self) -> None:
        # called once after collecting n_steps * n_envs transitions
        if self._count == 0:
            return
        for k in self.keys:
            mean_val = self._sum[k] / self._count
            self.logger.record(f"{self.log_prefix}/{k}", mean_val)
        # (optional) also log how many samples contributed
        self.logger.record(f"{self.log_prefix}/samples", int(self._count))

cb_rollout_mean = RolloutInfoMean(keys=("queues_mean", "collisions_count"))

start_time = time.time()
model.learn(total_timesteps=100_000, tb_log_name="run1", callback=cb_rollout_mean)
# model.learn(total_timesteps=100_000, tb_log_name="run1", callback=[episodic_logger, cb_qmean, cb_coll])
# model.learn(total_timesteps=100_000, tb_log_name=f"QueuePPO_run_lr{learning_rate}_nsteps_{n_steps}_batch_size_{batch_size}_nepochs_{n_epochs}_vf_coef_{vf_coef}_maxGradNorm_{max_grad_norm}_device_{device}")
end_time = time.time()
print(f"Training time: {end_time - start_time:.2f} seconds")

# model.save("ppo_queue_trained.zip")

def greedy_longest_no_collision(obs: Dict[str, np.ndarray]) -> np.ndarray:
    """
    Deterministic baseline:
      1) For each robot, if its *current* queue has work and is not already reserved, set SERVE.
      2) For remaining robots, greedily assign distinct longest queues (by length), with tie-break by lower index.
      3) If nothing to do, IDLE.
    Action coding: 0,...=switch_to(k).
    """
    robots = np.asarray(obs["robots"], dtype=np.int64)   # shape [M]
    queues = np.asarray(obs["queues"], dtype=np.float32) # shape [N]
    M = robots.shape[0]
    N = queues.shape[0]
    actions = robots
    reserved_q = set()
    assigned_r = set()

    # Pass 1: serve locally if possible (and not already reserved)
    for r in range(M):
        loc = int(robots[r])
        if queues[loc] > 0 and loc not in reserved_q:
            actions[r] = loc # serve
            reserved_q.add(loc)
            assigned_r.add(r)
    # Pass 2: assign longest remaining queues to unassigned robots
    for r in range(M):
        if r in assigned_r:
            continue
        pos_cands = [j for j in range(N) if queues[j] > 0 and j not in reserved_q]  # iterate over N queues
        if pos_cands:
            j_best = max(pos_cands, key=lambda j: (queues[j], -j))
            actions[r] = j_best  # switch_to(j_best)
            reserved_q.add(j_best)
        else:
            actions[r] = int(robots[r])  # idle
    return actions

def rollout_collect_mean_q(env_kwargs: Dict[str, Any],
                           seed: int,
                           T: int,
                           policy_fn) -> Dict[str, Any]:
    
    env = QueueEnv(**env_kwargs)
    # env = VecNormalize(env, norm_obs=True, norm_reward=True, clip_obs=10.)
    obs, _ = env.reset(seed=seed)

    means = []
    for t in range(T):
        a = policy_fn.predict(obs, deterministic=False)[0] if hasattr(policy_fn, "predict") else policy_fn(obs)
        # If the callable returned a tuple/list, assume it is SB3.predict-style: (action, states)
        # if t % 500 == 0:
        #     if hasattr(policy_fn, "predict"):
        #         # Optional debug: inspect logits of the underlying policy net
        #         with torch.no_grad():
        #             queues = torch.as_tensor(obs["queues"], dtype=torch.float32, device=device).unsqueeze(0)  # [1, N]
        #             robots = torch.as_tensor(obs["robots"], dtype=torch.long, device=device).unsqueeze(0)      # [1, M]
        #             obs_t = {"queues": queues, "robots": robots}
        #             logits, _ = model.policy.net(obs_t)
        #         print("logits[0] per robot:", logits[0])  # shape [M, N+2]
        #         print("max head index per robot:", logits[0].argmax(dim=-1))
        #         print("idle/serve vs switch mean:", logits[0][..., :2].mean(), logits[0][..., 2:].mean())
        obs, reward, terminated, truncated, info = env.step(a)
        # print(f"Step {t}: observations = {obs}")
        # print(f"Step {t}: action = {a}")
        means.append(float(info["queues_mean"]))
        # Basic action validity check (debug)
        if not (np.issubdtype(a.dtype, np.integer) and a.shape == (env.M,)):
            raise RuntimeError(f"Invalid action shape/dtype from policy: shape {a.shape}, dtype {a.dtype}")
        if np.any(a < 0) or np.any(a > env.N + 1):
            raise RuntimeError(f"Invalid action values from policy: {a}")
        if terminated or truncated:
            obs, _ = env.reset()

    means = np.asarray(means, dtype=np.float64)
    return {"means": means, "overall_mean": float(means.mean())}

def compare_policies_mean_q(env_kwargs: Dict[str, Any],
                            seed: int = 123,
                            T: int = 10_000,
                            model: "PPO" = None):
    
    greedy_pi = greedy_longest_no_collision
    # ppo_pi = lambda obs: model.predict(obs, deterministic=True)[0]

    greedy_res = rollout_collect_mean_q(env_kwargs, seed=seed, T=T, policy_fn=greedy_pi)
    ppo_res    = rollout_collect_mean_q(env_kwargs, seed=seed, T=T, policy_fn=model)

    print("\n=== Mean queue length over time (pre-step) ===")
    print(f"Greedy baseline:  overall mean = {greedy_res['overall_mean']:.6f}")
    print(f"PPO policy:       overall mean = {ppo_res['overall_mean']:.6f}")
    # quick sanity peek at the first few points
    k = 10
    print(f"\nFirst {k} pre-step means (Greedy): {np.array2string(greedy_res['means'][-k:], precision=3)}")
    print(f"First {k} pre-step means (PPO)   : {np.array2string(ppo_res['means'][-k:], precision=3)}")
    return greedy_res, ppo_res

eval_env_kwargs = dict(
    M=2, N=4,
    arrival_params=np.array([0.1, 0.1, 0.1, 0.1], dtype=np.float32),
    lambda_collision=2.0,
    max_steps_per_run=10_000,
)

# loaded = PPO.load("ppo_queue_trained.zip", device="cpu")

greedy_res, ppo_res = compare_policies_mean_q(
    env_kwargs=eval_env_kwargs,
    seed=123,
    T=100_000,
    model=model,
)

# # ---------- one-step trace (pure function; does NOT mutate env) ----------
# def step_trace(env: "QueueEnv",
#                obs: Dict[str, np.ndarray],
#                action: np.ndarray) -> Dict[str, Any]:
#     """
#     Mirrors QueueEnv.step() logic without mutating env.
#     Inputs:
#       env   : your QueueEnv (for params, rng)
#       obs   : dict with keys "robots":[M], "queues":[N]  (numpy arrays)
#       action: np.ndarray [M] with 0=idle, 1=serve, 2..=switch_to(k)

#     Returns a dict containing pre/post state, arrivals, who served, projections, reward, etc.
#     """
#     robots = np.asarray(obs["robots"], dtype=np.int64)      # [M]
#     queues = np.asarray(obs["queues"], dtype=np.float32)    # [N]
#     M, N = robots.shape[0], queues.shape[0]

#     # --- snapshot BEFORE
#     robots_before = robots.copy()
#     queues_before = queues.copy()

#     # --- normalize: "switch-to-self" -> serve  (same place as env)
#     act = np.asarray(action, dtype=np.int64).copy()
#     for r in range(M):
#         if act[r] >= 2:
#             k = act[r] - 2
#             if k == robots[r]:
#                 act[r] = 1

#     # --- reward from current queues (pre-cost), and served mask
#     cost_pre = float(queues_before.sum())
#     reward = -cost_pre
#     served_mask = np.zeros(M, dtype=np.int64)
#     queues_after_service = queues_before.copy()
#     for r in range(M):
#         if act[r] == 1:
#             loc = int(robots[r])
#             if queues_after_service[loc] > 0:
#                 queues_after_service[loc] -= 1.0
#                 served_mask[r] = 1

#     # --- intended positions from actions
#     intended = robots_before.copy()
#     for r in range(M):
#         a = int(act[r])
#         if a >= 2:
#             k = a - 2
#             if 0 <= k < N:
#                 intended[r] = k

#     # --- apply the SAME collision resolution as in QueueEnv._resolve_collisions
#     final_pos = robots_before.copy()
#     taken = set(final_pos.tolist())
#     num_proj = 0
#     for r in range(M):
#         tgt = int(intended[r])
#         cur = int(robots_before[r])

#         if tgt == cur:
#             # stayer: already reserved
#             continue

#         if tgt not in taken:
#             # move succeeds: free old spot, claim target
#             final_pos[r] = tgt
#             taken.remove(cur)
#             taken.add(tgt)
#         else:
#             # move fails: stay; count projection
#             num_proj += 1
#             # final_pos[r] stays 'cur'

#     # --- arrivals (use env RNG and its arrival params)
#     p = env.arrival_params["p"]
#     arrivals = env.rng.binomial(n=1, p=p, size=N).astype(np.float32)

#     # --- update queues with arrivals (after service)
#     queues_after = np.clip(queues_after_service + arrivals, 0.0, 1e6)

#     # --- collision penalty
#     if env.lambda_collision > 0 and num_proj > 0:
#         reward -= env.lambda_collision * float(num_proj)

#     next_obs = {"robots": final_pos.copy(), "queues": queues_after.copy()}
#     return {
#         "robots_before": robots_before,
#         "queues_before": queues_before,
#         "action": act.copy(),
#         "served_mask": served_mask,
#         "intended_positions": intended,
#         "robots_after": final_pos.copy(),
#         "arrivals": arrivals.astype(int),
#         "num_projections": int(num_proj),
#         "cost_pre": cost_pre,
#         "reward": reward,
#         "next_obs": next_obs,
#     }

# # ---------- rollout with pretty print ----------
# PolicyFn = Callable[[Dict[str, np.ndarray]], np.ndarray]

# def rollout_with_trace(env: "QueueEnv",
#                        policy: PolicyFn,
#                        T: int = 25,
#                        seed: Optional[int] = None,
#                        reset: bool = True,
#                        verbose: bool = True) -> List[Dict[str, Any]]:
#     """
#     Runs a T-step simulated trace (does not mutate env state).
#     If reset=True, starts from env.reset(seed); else starts from env._get_obs().
#     """
#     records: List[Dict[str, Any]] = []
#     if reset:
#         obs, _ = env.reset(seed=seed)
#     else:
#         obs = env._get_obs()

#     for t in range(T):
#         a = policy(obs)  # np.ndarray [M], int64
#         tr = step_trace(env, obs, a)
#         if verbose:
#             rb = tr["robots_before"]
#             qb = tr["queues_before"]
#             ra = tr["robots_after"]
#             arr = tr["arrivals"]
#             sm = tr["served_mask"]
#             inten = tr["intended_positions"]
#             print(
#                 f"t={t:03d}   robots:{rb}  queues:{qb}  a:{tr['action']}  "
#                 f"serve:{sm}  intend:{inten}  robots'→{ra}  "
#                 f"arr:{arr}  pre_cost:{tr['cost_pre']:.1f}  "
#                 f"r:{tr['reward']:.1f}  proj:{tr['num_projections']}"
#             )
#         records.append(tr)
#         obs = tr["next_obs"]

#     return records

# # Example quick check
# env = QueueEnv(
#     M=2, N=4,
#     arrival_params={"p": np.array([0.4, 0.1, 0.5, 0.5], dtype=np.float32)},
#     lambda_collision=3.0,
#     max_steps_per_run=10,
#     seed=123,
# )

# # Your baseline policy (make sure the candidate list uses N, not M)
# def greedy_longest_no_collision(obs: Dict[str, np.ndarray]) -> np.ndarray:
#     robots = np.asarray(obs["robots"], dtype=np.int64)
#     queues = np.asarray(obs["queues"], dtype=np.float32)
#     M, N = robots.shape[0], queues.shape[0]
#     actions = np.zeros(M, dtype=np.int64)
#     reserved_q, assigned_r = set(), set()
#     # serve locally
#     for r in range(M):
#         loc = int(robots[r])
#         if queues[loc] > 0 and loc not in reserved_q:
#             actions[r] = 1
#             reserved_q.add(loc)
#             assigned_r.add(r)
#     # assign distinct longest remaining queues
#     cand_idx = [j for j in range(N) if queues[j] > 0 and j not in reserved_q]  # NOTE: N here
#     cand_idx.sort(key=lambda j: (-queues[j], j))
#     ci = 0
#     for r in range(M):
#         if r in assigned_r:
#             continue
#         if ci >= len(cand_idx):
#             actions[r] = 0
#             continue
#         target = cand_idx[ci]; ci += 1
#         actions[r] = 1 if target == int(robots[r]) else 2 + target
#     return actions

# _ = rollout_with_trace(env, greedy_longest_no_collision, T=20, seed=123, verbose=True)

# obs, _ = env.reset()
# for _ in range(1000):
#     action, _ = model.predict(obs, deterministic=True)
#     obs, reward, terminated, truncated, info = env.step(action)
#     if terminated or truncated:
#         obs, _ = env.reset()

# print("SB3 wrote to:", model.logger.dir)